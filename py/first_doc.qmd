---
title: "My First Quarto Document"
author: "Your Name Here"
date: "2025-09-29"
format: html
---

# Introduction

This is my very first Quarto document!\
It will generate a PDF when I render it.

```{python}
print("Hello world!")
```

# Python Example

Here’s a Python code block that prints `"Hello World!"`:

```{python}
hello_world = "Hello world!"
print(hello_world)
```

```{python}
my_age = 21
current_year = 2025
year_of_birth = current_year - my_age
my_age = 50
print (my_age)
```

```{python}
my_age = 21
current_year = 2025
year_of_birth = current_year - my_age
print(year_of_birth)

to_kid = 30 - my-age
my_age + to-kid + 18
```

```{python}
num_1 = 1
type(num_1)
num_2 = 1.5
type(num_2)
```

```{python}
100 + 37
4 * 80
100 / 25
2 ** 2 
2 ** 2 == 2 * 2
10 % 3
n = 17

if n % 2 == 0:
    print("even")
else:
    print("odd")
    
10 // 3   # 3  (3 goes into 10 three times, remainder dropped)
14 // 5   # 2  (5 goes into 14 two times, remainder dropped)

14 / 5      # 2.8 (normal division)
14 % 5      # 4   (modulus)
14 // 5     # 2   (floor division)
```

```{python}
100 / 9
100 % 9
100 // 9

365 // 7
365 % 7
```

```{python}
title = "Dr."
first_name = "Andrew"
last_name = "Byrd"
title + first_name + last_name
print(title, first_name, last_name)
```

```{python}
print("I said, \"Hello!\"")
```

```{python}
first_name
first_name == "Bob"
# exactly equal to

first_name != "Bob"
# not equal to
```

```{python}
word = 'pin'
starts_p = word[0] == 'p'
starts_t = word[0] == 't'
starts_k = word[0] == 'k'
aspirated = starts_p or starts_t or starts_k
print(word: 'is aspirated', aspirated)
```

```{python}
novels = ["Sense and Sensibility", "Pride and Prejudice", "Mansfield Park", "Emma", "Northanger Abbey", "Persuasion", "Lady Susan"]
print(novels)

unfinished = ["The Watsons", "Sanditon"]
novels + unfinished

novels[6]

novels.index("Lady Susan")

pub_year = [1811, 1813, 1814, 1815, 1818, 1818, 1871]
susan_index = novels.index("Lady Susan")
pub_year[susan_index]  # 1871

year_index = pub_year.index(1814)
novels[year_index]
# 'Mansfield Park'

novels[-1]
# 'Lady Susan'
```

```{python}
orwell_novels = ["Animal Farm", "Nineteen Eighty-Four", "Burmese Days", "Keep the Aspidistra Flying", "Coming Up for Air"]
pub_year = [1945, 1949, 1934, 1936, 1939]

orwell_novels[0]
orwell_novels[-1]

pub_year[orwell_novels.index("Burmese Days")]

orwell_novels[pub_year.index(1936)]
```

```{python}
novels[0:3]
# ['Sense and Sensibility', 'Pride and Prejudice', 'Mansfield Park']

"Emma" in novels          # True
"Frankenstein" in novels  # False

novel_to_check = "Lady Susan"
novel_to_check in novels
```

```{python}
complex_list = [["Sense and Sensibility", 1811],
                ["Pride and Prejudice", 1813],
                ["Mansfield Park", 1814],
                ["Emma", 1815]]
complex_list[0]      # ["Sense and Sensibility", 1811]
complex_list[2][1]   # 1814
```

```{python}
novel_dict = {"Sense and Sensibility": 1811,
              "Pride and Prejudice": 1813,
              "Mansfield Park": 1814}
              
austen_dict = {
    "Sense and Sensibility": 1811,
    "Pride and Prejudice": 1813,
    "Mansfield Park": 1814,
    "Emma": 1815,
    "Northanger Abbey": 1818,   # duplicate value
    "Persuasion": 1818,         # duplicate value
    "Lady Susan": 1871
}

austen_dict["Pride and Prejudice"]
# 1813

austen_dict["The Rise of Han Solo"] = 2025

austen_dict["The Rise of Han Solo"] = 1872

del austen_dict["The Rise of Han Solo"]

"Pride and Prejudice" in austen_dict  # True
1811 in austen_dict                   # False
```

```{python}
orwell_dict = {
    "Animal Farm": 1945,
    "Nineteen Eighty-Four": 1949,
    "Burmese Days": 1934,
    "Keep the Aspidistra Flying": 1936,
    "Coming Up for Air": 1939
    }
    
orwell_dict['Animal Farm']

'Homage to Catalonia' in orwell_dict

orwell_dict['Homage to Catalonia'] = 1937

orwell_dict['Homage to Catalonia'] = 1938

del orwell_dict ['Coming Up for Air']

print(orwell_dict)
```

```{python}
x = 50
if x % 2 == 0: print(x, "is even")      # 50 is even

x % 2       # 0
x % 2 == 0  # True 
```

```{python}
if x % 2 == 0: 
    print(x, "is even.") 
    print("This line only prints if it's even too.") 
print("This line prints no matter what.") 
```

```{python}
if x == 51: 
    print(x, "is even.") 
    print("This line only prints if it's even too.") 
print("This line prints no matter what.") 
```

```{python}
if x % 2 == 0: 
    print(x, "is even") 
else: 
    print(x, "is odd") 
```

```{python}
x = 50.1
x % 2
```

```{python}
x = 50
if x % 2 == 0:
    print(x, "is even.")
elif x % 2 == 1:
    print(x, "is odd.")
else:
    print(x, "is a decimal.")
```

```{python}
charlotte = ["The Professor", "Jane Eyre", "Shirley", "Villette"] 
emily = ["Wuthering Heights"] 
anne = ["Agnes Grey", "The Tenant of Wildfell Hall"] 

bronte_checks = {"charlotte": 0, "emily": 0, "anne" : 0}  #a starter dict, where the values of each bronte book is at 0

if novel in charlotte: 
    bronte_checks["charlotte"] = bronte_checks["charlotte"] + 1
    print("Charlotte Brontë wrote", novel) 
elif novel in emily: 
    bronte_checks["emily"] = bronte_checks["emily"] + 1
    print("Emily Brontë wrote", novel) 
elif novel in anne: 
    bronte_checks["anne"] = bronte_checks["anne"] + 1
    print("Anne Brontë wrote", novel) 
else: print(novel, "was not written by one of the Brontë sisters")

print(bronte_checks)
```

```{python}
novel = "Wuthering Heights" 

charlotte = ["The Professor", "Jane Eyre", "Shirley", "Villette"] 
emily = ["Wuthering Heights"] 
anne = ["Agnes Grey", "The Tenant of Wildfell Hall"] 

bronte_checks = {"charlotte": 0, "emily": 0, "anne" : 0} 

if novel in charlotte: 
    bronte_checks["charlotte"] = bronte_checks["charlotte"] + 1
    print("Charlotte Brontë wrote", novel) 
elif novel in emily: 
    bronte_checks["emily"] = bronte_checks["emily"] + 1
    print("Emily Brontë wrote", novel)
elif novel in anne: 
    bronte_checks["anne"] = bronte_checks["anne"] + 1
    print("Anne Brontë wrote", novel) 
else: print(novel, "was not written by one of the Brontë sisters")

print(bronte_checks)
```

```{python}

sound = ('k')

vowels = ["a", "e", "i", "o", "u"]
consonants = ["p", "t", "k", "m", "n", "s", "r", "l"]

if sound in consonants:
    print(sound, "is a consonant")
elif sound in vowels:
    print(sound, "is a vowel")
else: print(sound, "is something else")
```

```{python}
word = "bʌs"

sibilants = ["s", "z", "ʃ", "ʒ", "tʃ", "dʒ"] 
voiceless = ["p", "t", "k", "f", "θ"]

last_letter = word[-1]

if last_letter in sibilants:
    plural = word + "ɪz"
elif last_letter in voiceless:
    plural = word + "s"
else: plural = word + "z"

print(last_letter)
print(plural)
```

```{python}
charlotte = ['The Professor', 'Jane Eyre', 'Shirley', 'Villette']

print(charlotte[0])
print(charlotte[1])
print(charlotte[2])
print(charlotte[3])
#or, if you don't know how many
print(len(charlotte))

for title in charlotte: 
    print(title)
    
for book in charlotte: 
    print(book)
```

```{python}
charlotte = ['The Professor', 'Jane Eyre', 'Shirley', 'Villette']
for book in charlotte: 
    print(book)
print(book)
```

```{python}
words = ["phoneme", "phrase", "morpheme", "reconstruction", "index"]

for term in words:
    print(term, "has", len(term), "letters")
```

```{python}
import os
print(os.getcwd())

import urllib.request

url = "https://www.gutenberg.org/files/141/141-0.txt"  # Mansfield Park
filename = "mansfield_park.txt"

urllib.request.urlretrieve(url, filename)

print("Downloaded:", filename)

with open("mansfield_park.txt", "r", encoding="utf-8") as f:
    book_text = f.read()
    
print(book_text[:200])
```

```{python}
with open("mansfield_park.txt", "r", encoding="utf-8") as f:
    book_lines = f.readlines()

len(book_lines)

book_lines[101]
```

```{python}
one_line = book_lines[101]
one_line.rstrip()
```

```{python}
words = [" Cat\n", "dog", " BIRD ", "fish\n", "LION", "tiger ", "bear", "OWL\n"]

for word in words:
    cleaned = word.strip().lower()
    print(cleaned)
```

```{python}
test_list = ["a", "b", "c"]
test_list.append("d")

print(test_list)
```

```{python}
words = [" Cat\n", "dog", " BIRD ", "fish\n", "LION", "tiger ", "bear", "OWL\n"]
cleaned_words = []

for word in words:
    cleaned = word.strip().lower()
    cleaned_words.append(cleaned)

print(cleaned_words)
```

```{python}
words = [" Cat\n", "dog", " BIRD ", "fish\n", "LION", "tiger ", "bear", "OWL\n"]

cleaned_words = [word.strip().lower() for word in words]

print(cleaned_words)
```

```{python}
items = ["color", "flavour", "theater", "center", "analyze", "organize", "favorite", "neighbor", "honor", "catalog", "honour", "flavor", "analyse", "flavor", "traveler"]
print(len(items))
```

```{python}
items = ["color", "flavour", "theater", "center", "analyze", "organize", "favorite", "neighbor", "honor", "catalog", "honour", "flavor", "analyse", "flavor", "traveler"]

item_count = 0
for item in items:
    item_count += 1
    print(item, item_count)
```

```{python}
items = ["color", "flavour", "theater", "center", "analyze", "organize", "favorite", "neighbor", "honor", "catalog", "honour", "flavor", "analyse", "flavor", "traveler"]

c_item_count = 0
for item in items:
    if item[0] == "c":
        c_item_count += 1
        print(item, c_item_count)
```

```{python}
items = ["color", "flavour", "theater", "center", "analyze", "organize", "favorite", "neighbor", "honor", "catalog", "honour", "flavor", "analyse", "flavor", "traveler"]

r_item_count = 0
for item in items:
    if item[-1] == "r":
        r_item_count += 1
        print(item, r_item_count)
```

```{python}
word = "word"
word[0]
numb = 0

word = "four"
new_word  = word.replace("our", "or")
print(new_word)
```

```{python}
items = ["color", "flavour", "theater", "center", "analyze", "organize", "favorite", "neighbor", "honor", "catalog", "honour", "flavor", "analyse", "flavor", "traveler"]

down_with_brits = []
for word in items:
    if word.endswith("our"):
        word = word.replace("our", "or")
    elif word.endswith("yse"):
        word = word.replace("yse", "yze")
    down_with_brits.append(word)

print(down_with_brits)
```

```{python}
getty = "Four score and seven years ago our fathers brought forth on this continent, a new nation, conceived in Liberty, and dedicated to the proposition that all men are created equal."

getty.split()
```

```{python}
getty = "Four score and seven years ago our fathers brought forth on this continent, a new nation, conceived in Liberty, and dedicated to the proposition that all men are created equal."

getty_1 = getty.split(",")
getty_2 = getty.split(".")
getty_3 = getty.split("e")
print(getty_1, "\n", getty_2, "\n", getty_3)
```

```{python}
import re

text = "Hello, world!   This... is a test."
words = re.split(r"[\s\W]+", text)
words = [w for w in words if w] 
print(words)
# \s = whitespace
# \W = non-word characters (punctuation, symbols, etc.)
# + = one or more preceding
```

```{python}
with open("alice.txt", "r", encoding="utf-8") as f:
    alice_text = f.read()
    
print(alice_text[:100])
```

```{python}
word = "computation"
vowels = "aeiou"

for c in word:
    if c in vowels:
        print(c, "is a vowel")
    else: 
        print (c, "is not a vowel")
```

```{python}
import pandas as pd

langs = pd.Series(["Latin", "Greek", "Sanskrit"], name="language")
langs 
```

```{python}
import pandas as pd

langs = pd.Series(["Latin", "Greek", "Sanskrit", "Finnish"], index=["a","b","c","d"], name="language")
langs
```

```{python}
import pandas as pd

lang_df = pd.DataFrame(langs)
lang_df
```

```{python}
import pandas as pd

fam = pd.Series(["Indo-European", "Indo-European", "Indo-European", "Uralic"], index=["a","b","c","d"], name="family")
fam
```

```{python}
import pandas as pd
speakers = pd.Series([0, 0, 0, 6], index=["a","b","c","d"], name="speakers_millions")
speakers
```

```{python}
import pandas as pd

langs = pd.Series(["Latin", "Greek", "Sanskrit", "Finnish"], index=["a","b","c","d"], name="language")
fam = pd.Series(["Indo-European", "Indo-European", "Indo-European", "Uralic"], index=["a","b","c","d"], name="family")
speakers = pd.Series([0, 0, 0, 6], index=["a","b","c","d"], name="speakers_millions")

languages_df = pd.concat([langs, fam, speakers], axis=1)
print(languages_df)
```

```{python}
import pandas as pd

location = pd.Series(["Italy", "Greece", "India", "Finland"], index=["a","b","c","d"], name="location")
new_df = pd.concat([languages_df, location], axis=1)

print(new_df)
```

```{python}
import pandas as pd

fresh_df = pd.DataFrame({
    "language": ["Latin", "Greek", "Sanskrit", "Finnish"],
    "family":   ["Indo-European", "Indo-European", "Indo-European", "Uralic"],
    "speakers_millions": [0, 0, 0, 6],
    "location": ["Italy", "Greece", "India", "Finland"]
}, index=["a","b","c","d"])

print(fresh_df)
```

```{python}
import pandas as pd

phonemes = pd.DataFrame({
    "sound": ["/p/", "/b/", "/t/", "/d/", "/s/", "/h/"],
    "voice":   ["voiceless", "voiced", "voiceless", "voiced", "voiceless", "voiceless"],
    "place": ["bilabial", "bilabial", "alveolar", "alveolar", "alveolar", "glottal"],
    "manner": ["stop", "stop", "stop", "stop", "fricative", "fricative"]
}, index=["0","1","2","3", "4", "5"])

print(phonemes)
```

```{python}
import pandas as pd

gettysburg = ["Four", "score", "and", "seven", "years", "ago"]
getty = pd.Series(gettysburg)

print(getty)
```

```{python}
import pandas as pd

languages = {
    "French": ["Romance"],
    "Spanish": ["Romance"],
    "Italian": ["Romance"],
    "English": ["Germanic"]
}
lang_df = pd.DataFrame(languages)
print(lang_df)
```

```{python}
import pandas as pd

languages = {
    "French": ["Romance", 1, "b"],
    "Spanish": ["Romance", 1, "c"],
    "Italian": ["Romance", 4, "a"],
    "English": ["Germanic", 10, "x"]
}
lang_df = pd.DataFrame(languages)
print(lang_df)
```

```{python}
import pandas as pd

languages = {
    "French": "Romance",
    "Spanish": "Romance",
    "Italian": "Romance",
    "English": "Germanic"
}

lang_df = pd.DataFrame.from_dict(languages, orient="index", columns=["Family"])
print(lang_df)
```

```{python}
import pandas as pd

languages = {
    "French": "Romance",
    "Spanish": "Romance",
    "Italian": "Romance",
    "English": "Germanic"
}

lang_df = pd.DataFrame(list(languages.items()), columns=["Language", "Family"])
print(lang_df)
```

```{python}
import pandas as pd

prim_loc = pd.Series(["France", "Spain", "Italy", "United Kingdom"], name="Primary location")
num_speakers = pd.Series(["320 million", "500 million", "70 million", "1500 million"], name="# of speakers")
writing_sys = pd.Series(["Latin alphabet", "Latin alphabet", "Latin alphabet", "Latin alphabet"], name="Writing system")
iso = pd.Series(["fr", "es", "it", "en"], name="ISO code")

lang_updated = pd.concat([lang_df, prim_loc, num_speakers, writing_sys, iso], axis=1)
lang_updated
```

```{python}
import pandas as pd

pie_df = pd.DataFrame({
    "Root": ["*bʰer-", "*genh₁-", "*ped-", "*doh₃-"],
    "Meaning":   ["'to carry'", "'to beget'", "'foot'", "'to give'"],
    "Latin": ["ferō", "gignō", "pēs", "dō"],
    "Greek": ["pʰérō", "gígnomai", "poús", "dídōmi"],
    "English": ["bear", "kin", "foot", "donate"]
}, index=["0","1","2","3"])

pie_df
```

```{python}
import pandas as pd

fresh_df = pd.DataFrame({
    "language": ["Latin", "Greek", "Sanskrit", "Finnish"],
    "family":   ["Indo-European", "Indo-European", "Indo-European", "Uralic"],
    "speakers_millions": [0, 0, 0, 6],
    "location": ["Italy", "Greece", "India", "Finland"]
})

print(fresh_df)

print("------------")
fresh_df.head(2) 
print("------------")
fresh_df.tail(2) 
print("------------")
fresh_df.shape
print("------------")
fresh_df.columns
print("------------")
fresh_df.index
print("------------")
fresh_df.info()
```

```{python}
fresh_df["language"]           # Series
print("------------")
fresh_df[["language","family"]] # DataFrame
print("------------")
fresh_df.iloc[0]               # first row
print("------------")
fresh_df.iloc[1:3]             # slice rows 1..2
```

```{python}
import pandas as df
# Rows by label (after we set a custom index)
df2 = fresh_df.copy()
df2.index = ["a","b","c","d"]
df2.loc["b"]             # row labeled 'b'
```

```{python}
fresh_df.loc[0, "language"]        # cell by label/column
print("------------")
fresh_df.iloc[0, 0]                # cell by position
print("------------")
fresh_df.iloc[0:3, 0:2]            # rows 0..2, cols 0..1
```

```{python}
df = fresh_df.copy()
ie = df[df["family"] == "Indo-European"]
ie
print("------------")
ie2 = (df["family"]=="Indo-European") & (df["location"]=="India")
ie2
```

```{python}
df[(df["family"]=="Indo-European") & (df["location"]=="India")]
print("------------")
df[(df["family"]=="Indo-European") | (df["location"]=="Finland")]
```

```{python}
df[~(df["family"]=="Indo-European")]
print("------------")
df.sort_values(["location"])
print("------------")
df["speakers_millions"].sum()
print("------------")
int(df["speakers_millions"].sum())
```

```{python}
import pandas as pd

eng_vowels = pd.DataFrame({
    "symbol": ["i", "ɪ", "e", "ɛ", "æ", "u", "ʊ", "o", "ɔ", "ɑ", "ʌ", "ə"],
    "height": ["high", "high", "mid", "mid", "low", "high", "high", "mid", "mid", "low", "mid", "mid"], #choose from "high", "mid", and "low"
    "backness": ["front", "front", "front", "front", "front", "back", "back", "back", "back", "back", "central", "central"], # choose from "front", "back", and "central"
    "tense":   ["true", "false", "true", "false", "false", "true", "false", "true", "false", "false", "false", "false"], #choose from True or False
    "rounded": ["false", "false", "false", "false", "false", "true", "true", "true", "true", "false", "false", "false"], #choose from True or False
})

eng_vowels
```

```{python}
eng_vowels[(eng_vowels["height"]=="high") & (eng_vowels["backness"]=="back") & (eng_vowels["rounded"]=="true")]
```

```{python}
eng_vowels[~(eng_vowels["tense"]=="true")]
```

```{python}
eng_vowels[~(eng_vowels["tense"]=="true")].head(3)
```

```{python}
eng_vowels.sort_values(["height", "backness"])
```

```{python}
rounded = eng_vowels["rounded"].sum()
rounded
```

```{python}
name = "Bob, Susie, Jimbo"
prof = "teacher, doctor, lawyer"
age = "45, 63, 119"

name_split = name.split(", ")
prof_split = prof.split(", ")
age_split = age.split(", ")

print(name_split, prof_split, age_split)
```

```{python}
import pandas as pd

recap_df = pd.DataFrame([prof_split, age_split], columns=name_split)
recap_df
```

```{python}
import pandas as pd

line1 = "English German Spanish French"
line2 = "dog Hund perro chien"
line3 = "cat Katze gato chat"
line4 = "house Haus casa maison"

l1s = line1.split()
l2s = line2.split()
l3s = line3.split()
l4s = line4.split()

print(l1s, l2s, l3s, l4s)

print("-----------------")

line_df = pd.DataFrame([l2s, l3s, l4s], columns=l1s)
line_df
```

```{python}
import pandas as pd

df = pd.DataFrame({
    "language": ["Latin", "Greek", "Sanskrit", "Finnish"],
    "family":   ["Indo-European", "Indo-European", "Indo-European", "Uralic"],
    "speakers_millions": [0, 0, 0, 6],
    "location": ["Italy", "Greece", "India", "Finland"]
})

df
```

```{python}
df["is_extinct"] = [True, True, True, False]
df
```

```{python}
# Drop (returns a new DF unless inplace=True)
df_no_family = df.drop(columns=["family"])
df_no_family
```

```{python}
# Drop (returns a new DF unless inplace=True) 
df_no_latin = df.drop([0])
df_no_latin
```

```{python}
df = df.rename(columns={"speakers_millions": "speakers_M"})
df
```

```{python}
df_reset_no_latin = df_no_latin.reset_index()
df_reset_no_latin
```

```{python}
df_ind_drop = df.reset_index(drop=True)
df_ind_drop
```

```{python}
df_set = df.set_index("language")
df_set
```

```{python}
import pandas as pd 

line_df

print("-----------------")

no_dog = line_df.drop([0])
no_dog

print("-----------------")

reset_line_df = no_dog.reset_index(drop=True)
reset_line_df
```

```{python}
import pandas as pd

df = pd.DataFrame({
    "language": ["Latin", "Greek", "Sanskrit", "Finnish"],
    "family":   ["Indo-European", "Indo-European", "Indo-European", "Uralic"],
    "speakers_M": [0, 0, 0, 6],
    "location": ["Italy", "Greece", "India", "Finland"]
})

df.sort_values(by="location", ascending=True)
```

```{python}
df.sort_index() # by index
```

```{python}
reflexes_verb = pd.Series(
    {"Latin": 8, "Greek": 10, "Sanskrit": 12, "Old English": 6}
)
reflexes_noun = pd.Series(
    {"Latin": 5, "Greek": 7, "Gothic": 4, "Old Church Slavonic": 3}
)

combined = pd.DataFrame({"verb": reflexes_verb, "noun": reflexes_noun})
combined
```

```{python}
combined_filled = combined.fillna(0)
combined_filled
```

```{python}
combined_dropped = combined.dropna()
combined_dropped
```

```{python}
total = reflexes_verb.add(reflexes_noun, fill_value=0)
total.sort_values(ascending=False)
```

```{python}

reflexes_verb = pd.Series(
    {"Latin": 8, "Greek": 10, "Sanskrit": 12, "Old English": 6}
)
reflexes_noun = pd.Series(
    {"Latin": 5, "Greek": 7, "Gothic": 4, "Old Church Slavonic": 3}
)
combined = pd.DataFrame({"verb": reflexes_verb, "noun": reflexes_noun})
combined

column_totals = combined.sum(axis=0)
print(column_totals)
```

```{python}
import pandas as pd

sound_df = pd.DataFrame({
    "s": [35, 28, 40],
    "z": [20, 25, 18],
    "t": [50, 40, 53],
    "k": [42, 35, 48],
    "m": [18, 12, 17],
    "n": [22, 20, 19]
})
sound_df
```

```{python}
import pandas as pd

total = sound_df["s"].add(sound_df["z"])
total

#finish this activity for Friday
```

```{python}
import pandas as pd

stops = {
    "Language": ["English", "Spanish", "Hindi"],
    "Nasals": [3, 3, 4],
    "Oral Stops": [6, 6, 5]
}

print(type(stops))
```

```{python}
import pandas as pd

stops_df = pd.DataFrame(stops)
stops_df
```

```{python}
import pandas as pd

stops_df = pd.DataFrame(stops).set_index("Language")

stops_df.sum()
totals = df["Nasals"].add(df["Oral Stops"])
totals
```

```{python}
import pandas as pd

# Create the data
data = {
    "Consonant": ["p", "b", "t", "d", "k", "g", "s", "z", "m", "n", "l", "r", "w", "j", "h"],
    "Place": ["bilabial", "bilabial", "alveolar", "alveolar", "velar", "velar",
              "alveolar", "alveolar", "bilabial", "alveolar", "alveolar", "alveolar",
              "labial-velar", "palatal", "glottal"],
    "Manner": ["stop", "stop", "stop", "stop", "stop", "stop",
               "fricative", "fricative", "nasal", "nasal", "lateral", "trill",
               "glide", "glide", "fricative"],
    "Voicing": ["voiceless", "voiced", "voiceless", "voiced", "voiceless", "voiced",
                "voiceless", "voiced", "voiced", "voiced", "voiced", "voiced",
                "voiced", "voiced", "voiceless"]
}

# Create DataFrame
df = pd.DataFrame(data)

# Show DataFrame
print(df)

# Save to CSV
df.to_csv("conlang_c.csv", index=False)
```

```{python}
stops_df.to_csv("stops.csv", index=False)
```

```{python}
import openpyxl
import pandas as pd

data = {
    "Consonant": ["p", "b", "t", "d", "k", "g", "s", "z", "m", "n", "l", "r", "w", "j", "h"],
    "Place": ["bilabial", "bilabial", "alveolar", "alveolar", "velar", "velar",
              "alveolar", "alveolar", "bilabial", "alveolar", "alveolar", "alveolar",
              "labial-velar", "palatal", "glottal"],
    "Manner": ["stop", "stop", "stop", "stop", "stop", "stop",
               "fricative", "fricative", "nasal", "nasal", "lateral", "trill",
               "glide", "glide", "fricative"],
    "Voicing": ["voiceless", "voiced", "voiceless", "voiced", "voiceless", "voiced",
                "voiceless", "voiced", "voiced", "voiced", "voiced", "voiced",
                "voiced", "voiced", "voiceless"]
}

# Create DataFrame
df = pd.DataFrame(data)

df.to_excel("conlang_c.xlsx", index=False)
```

```{python}
stops_csv_df = pd.read_csv("conlang_c.csv")
stops_csv_df.head()

print("\n-------------------------------------\n")

stops_excel_df = pd.read_excel("conlang_c.xlsx")
stops_excel_df.head()
```

```{python}
ref_df = pd.DataFrame({
    "Language": ["Latin","Greek","Sanskrit","Gothic","OCS","Oscan"],
    "Family":   ["Italic","Hellenic","Indic","Germanic","Slavic","Italic"],
    "Reflexes": [8,10,12,4,3,5]
})
ref_df
```

```{python}
int(ref_df["Reflexes"].sum())
```

```{python}
ref_df.groupby("Family")["Reflexes"].sum()
```

```{python}
ref_df.groupby("Family").agg(
    total=("Reflexes","sum"),
    mean=("Reflexes","mean"),
    n=("Reflexes","count")
)
```

```{python}
import pandas as pd

df = pd.DataFrame({
    "Language": ["English","German","Dutch","Spanish","Italian","French","Greek","Hindi","Bengali"],
    "Family":   ["Germanic","Germanic","Germanic","Romance","Romance","Romance","Hellenic","Indic","Indic"],
    "Consonants": [24, 25, 20, 17, 23, 22, 25, 33, 29],
    "Vowels": [20, 16, 13, 5, 7, 15, 7, 11, 7]
})
df
```

```{python}
df.groupby("Family").agg(
    total_c = ("Consonants","sum"),
    total_v = ("Vowels","sum"),
    mean_c = ("Consonants","mean"),
    mean_v = ("Vowels","mean"),
    n = ("Language", "count")
)
```

```{python}
import matplotlib.pyplot as plt

counts = ref_df.groupby("Family")["Reflexes"].sum()
ax = counts.plot(kind="bar", title="Reflex Counts by Family")
ax.set_xlabel("Family")
ax.set_ylabel("Total Reflexes")

plt.tight_layout()   # optional, avoids label cutoff
plt.show()           # displays the plot
```

```{python}
import matplotlib.pyplot as plt

con_c_df = pd.read_csv("conlang_c.csv")
stops_df = con_c_df[(con_c_df["Manner"] == "stop")]

stop_counts = stops_df["Place"].value_counts().sort_index()
stop_counts
```

```{python}
import matplotlib.pyplot as plt

con_c_df = pd.read_csv("conlang_c.csv")
stops_df = con_c_df[(con_c_df["Manner"] == "stop")]

stop_counts = stops_df["Place"].value_counts().sort_index()

ax = counts.plot(kind="bar", title="Consonant Counts by Place of Articulation")
ax.set_xlabel("Place")
ax.set_ylabel("Number of Consonant")

plt.tight_layout()   # optional, avoids label cutoff
plt.show()           # displays the plot
```

```{python}
import re

jfk = "This year’s space budget is three times what it was in January 1961, and it is greater than the space budget of the previous eight years combined. That budget now stands at $5,400,000 a year — a staggering sum, though somewhat less than we pay for cigarettes and cigars every year. Space expenditures will soon rise some more, from 40 cents per person per week to more than 50 cents a week for every man, woman and child in the United States, for we have given this program a high national priority — even though I realize that this is in some measure an act of faith and vision, for we do not now know what benefits await us. But if I were to say, my fellow citizens, that we shall send to the moon, 240,000 miles away from the control station in Houston, a giant rocket more than 300 feet tall, the length of this football field, made of new metal alloys, some of which have not yet been invented, capable of standing heat and stresses several times more than have ever been experienced, fitted together with a precision better than the finest watch, carrying all the equipment needed for propulsion, guidance, control, communications, food and survival, on an untried mission, to an unknown celestial body, and then return it safely to Earth, re-entering the atmosphere at speeds of over 25,000 miles per hour, causing heat about half that of the temperature of the sun — almost as hot as it is here today — and do all this, and do it right, and do it first before this decade is out — then we must be bold. I’m the one who is doing all the work, so we just want you to stay cool for a minute. [laughter] However, I think we’re going to do it, and I think that we must pay what needs to be paid. I don’t think we ought to waste any money, but I think we ought to do the job. And this will be done in the decade of the sixties. It may be done while some of you are still here at school at this college and university. It will be done during the term of office of some of the people who sit here on this platform. But it will be done. And it will be done before the end of this decade. I am delighted that this university is playing a part in putting a man on the moon as part of a great national effort of the United States of America. Many years ago, the great British explorer George Mallory, who was to die on Mount Everest, was asked why did he want to climb it? He said, “Because it is there.” Well, space is there, and we’re going to climb it, and the moon and the planets are there, and new hopes for knowledge and peace are there. And, therefore, as we set sail we ask God’s blessing on the most hazardous and dangerous and greatest adventure on which man has ever embarked. Thank you."

jfk_words = re.split(r"[\s\W]+", jfk)
jfk_words = [word.lower() for word in jfk_words if word]

jfk_words
```

```{python}
len(jfk_words)
```

```{python}
import re

jfk_unique = {}

for word in jfk_words:
    if word in jfk_unique:
        jfk_unique[word] += 1
    else:
        jfk_unique[word] = 1

jfk_unique
```

```{python}
list1 = ["a", "b", "c", "d"]
list2 = [0, 1, 2, 3]

new_dict = {}

for i in list1:
  for j in list2:
    new_dict[i] = j

print(new_dict)
```

```{python}
import pandas as pd

list1 = ["a", "b", "c", "d"]
list2 = [0, 1, 2, 3]

new_dict = {}

new_dict = {i: [j] for i, j in zip(list1, list2)}

df = pd.DataFrame(new_dict)
df
```

```{python}
list1 = ["a", "b", "c", "d"]
list2 = [0, 1, 2, 3]
list3 = ["alpha", "beta", "gamma", "delta"]
list4 = ["fee", "fie", "fo", "fum"]


list2dict = {i: [j, k, l] for i, j, k, l in zip(list1, list2, list3, list4)}

list2dict
```

```{python}
import pandas as pd

df2 = pd.DataFrame(list2dict)
df2
```

```{python}
consonant =  ["p", "b", "t", "d", "k", "g", "s", "z", "m", "n", "l", "r", "w", "j", "h"]
place = ["bilabial", "bilabial", "alveolar", "alveolar", "velar", "velar", "alveolar", "alveolar", "bilabial", "alveolar", "alveolar", "alveolar", "labial-velar", "palatal", "glottal"]
manner = ["stop", "stop", "stop", "stop", "stop", "stop", "fricative", "fricative", "nasal", "nasal", "lateral", "trill", "glide", "glide", "fricative"]
voice = ["voiceless", "voiced", "voiceless", "voiced", "voiceless", "voiced", "voiceless", "voiced", "voiced", "voiced", "voiced", "voiced", "voiced", "voiced", "voiceless"]

lin2dict = {i: [j, k, l] for i, j, k, l in zip(consonant, place, manner, voice)}

lin_df = pd.DataFrame(lin2dict)
lin_df

lin_df.to_csv("lin_df.csv", index=False)
```

```{python}
list1 = ["a", "b", "c", "d"]
list2 = [0, 1, 2, 3]

new_dict = {}

for i in list1:
  for j in list2:
    new_dict[i] = j

print(new_dict)
```

```{python}
list = ["eeny", "meeny", "miney", "mo"]

for i in list:
  for c in i:
    print(c)
# i = items, c = characters
```

```{python}
words = ["eeny", "meeny", "miney", "mo"]
words_dict = {}

for w in words:
    vowels = 0               # temporary variable counter
    for c in w:              
        if c in "aeiou":     # check if the letter is a vowel
            vowels += 1      # increment the counter
    print(w, ":", vowels, "vowels")
    words_dict[w] = vowels
    
print(words_dict)
```

```{python}
sentences = [
    ["the", "cat", "sleeps"],
    ["the", "dog", "runs"],
    ["the", "bird", "flies"]
]

for sentence in sentences:
  for w in sentence:
    print(w)
```

```{python}
words = [
    ["lin", "guis", "tics"],
    ["pho", "no", "lo", "gy"],
    ["mor", "pho", "lo", "gy"]
]

syl_count = 0

for word in words:
    for syl in word:
        print(syl)
        syl_count += 1
        
syl_count
```

```{python}
list = ["eeny", "meeny", "miney", "mo"]
list[0][0]
```

```{python}
sentences = [
    ["the", "cat", "sleeps"],
    ["the", "dog", "runs"],
    ["the", "bird", "flies"]
]
sentences[0][0]
```

```{python}
consonants = {
    "p": {"voice": False, "place": "bilabial", "manner": "stop"},
    "t": {"voice": False, "place": "alveolar", "manner": "stop"},
    "k": {"voice": False, "place": "velar", "manner": "stop"}
}

students = {
    "Alice": {"quiz": 9, "homework": 10, "final": 8},
    "Ben": {"quiz": 7, "homework": 9, "final": 10}
}

students["Alice"]["quiz"]
```

```{python}
import pandas as pd
consonants = {
    "p": {"voice": False, "place": "bilabial", "manner": "stop"},
    "t": {"voice": False, "place": "alveolar", "manner": "stop"},
    "k": {"voice": False, "place": "velar", "manner": "stop"}
}

df = pd.DataFrame(consonants).T.reset_index(names="phoneme")
df
```

```{python}
import pandas as pd
consonants = {
    "p": {"voice": False, "place": "bilabial", "manner": "stop"},
    "t": {"voice": False, "place": "alveolar", "manner": "stop"},
    "k": {"voice": False, "place": "velar", "manner": "stop"}
}
df = pd.DataFrame(consonants)
df
```

```{python}
import pandas as pd

words   = ["students", "run", "ran", "roots", "languages"]
pos_tags = ["NOUN", "VERB", "VERB", "NOUN", "NOUN"]
lemmas  = ["student", "run", "run", "root", "language"]
freqs   = [5, 3, 2, 4, 6]

rev_activity = {}
for i, j, k, l in zip(words,pos_tags,lemmas,freqs):
  rev_activity[i] = {
    "pos_tags": j,
    "lemmas": k,
    "freqs": l
}

df_act = pd.DataFrame(rev_activity).T.reset_index(names="words")
df_act
```

```{python}
import spacy

# Load the English model
nlp = spacy.load("en_core_web_sm")

# Test text
text = "Hello world! SpaCy is working."

# Process the text
doc = nlp(text)

# Print out each token (word) and its part of speech
for token in doc:
    print(token.text, token.pos_)

```

```{python}
text = "Dr. Byrd's students can't wait to analyze PIE roots!"
tokens_basic = text.split()
tokens_basic
```

```{python}
import re

text = "Dr. Byrd's students can't wait to analyze PIE roots!"

tokens_clean = re.split(r"[\s\W]+", text)
tokens_clean
```

```{python}
import spacy
import pandas as pd

# Load English model
nlp = spacy.load("en_core_web_sm")

text = "Dr. Byrd's students can't wait to analyze PIE roots!"
doc = nlp(text)

# Create list of dicts, one per token
data = []
for t in doc:
    data.append({
        "text": t.text,
        "lemma": t.lemma_,
        "POS": t.pos_,
        "tag": t.tag_,
        "stop": t.is_stop,
        "is_punct": t.is_punct
    })

# Make DataFrame
df = pd.DataFrame(data)
print(df)
```

```{python}
import spacy
import pandas as pd

# Load English model
nlp = spacy.load("en_core_web_sm")

jfk = "This year’s space budget is three times what it was in January 1961, and it is greater than the space budget of the previous eight years combined. That budget now stands at $5,400,000 a year — a staggering sum, though somewhat less than we pay for cigarettes and cigars every year. Space expenditures will soon rise some more, from 40 cents per person per week to more than 50 cents a week for every man, woman and child in the United States, for we have given this program a high national priority — even though I realize that this is in some measure an act of faith and vision, for we do not now know what benefits await us. But if I were to say, my fellow citizens, that we shall send to the moon, 240,000 miles away from the control station in Houston, a giant rocket more than 300 feet tall, the length of this football field, made of new metal alloys, some of which have not yet been invented, capable of standing heat and stresses several times more than have ever been experienced, fitted together with a precision better than the finest watch, carrying all the equipment needed for propulsion, guidance, control, communications, food and survival, on an untried mission, to an unknown celestial body, and then return it safely to Earth, re-entering the atmosphere at speeds of over 25,000 miles per hour, causing heat about half that of the temperature of the sun — almost as hot as it is here today — and do all this, and do it right, and do it first before this decade is out — then we must be bold. I’m the one who is doing all the work, so we just want you to stay cool for a minute. [laughter] However, I think we’re going to do it, and I think that we must pay what needs to be paid. I don’t think we ought to waste any money, but I think we ought to do the job. And this will be done in the decade of the sixties. It may be done while some of you are still here at school at this college and university. It will be done during the term of office of some of the people who sit here on this platform. But it will be done. And it will be done before the end of this decade. I am delighted that this university is playing a part in putting a man on the moon as part of a great national effort of the United States of America. Many years ago, the great British explorer George Mallory, who was to die on Mount Everest, was asked why did he want to climb it? He said, “Because it is there.” Well, space is there, and we’re going to climb it, and the moon and the planets are there, and new hopes for knowledge and peace are there. And, therefore, as we set sail we ask God’s blessing on the most hazardous and dangerous and greatest adventure on which man has ever embarked. Thank you."

doc = nlp(jfk)

# Create list of dicts, one per token
data = []
for t in doc:
    data.append({
        "text": t.text,
        "lemma": t.lemma_,
        "POS": t.pos_,
        "tag": t.tag_,
        "stop": t.is_stop,
        "is_punct": t.is_punct
    })

# Make DataFrame
df = pd.DataFrame(data)
print(df)

```

```{python}
import spacy
nlp = spacy.load("en_core_web_sm")

doc = nlp("The children ran quickly to their houses.")

for token in doc:
    print(token.text, "→", token.lemma_, token.pos_)
```

```{python}
text = "Students were running, reading, and writing all day."
doc = nlp(text)

tokens = [t.text for t in doc]
lemmas = [t.lemma_ for t in doc]

list(zip(tokens, lemmas))
```

```{python}
nouns = [t.lemma_ for t in doc if t.pos_ == "NOUN"]
verbs = [t.lemma_ for t in doc if t.pos_ == "VERB"]

print("Nouns:", nouns)
print("Verbs:", verbs)
```

```{python}
import pandas as pd
from collections import Counter

text = "Students were running, reading, and writing all day, student."
sentence = nlp(text)

lemmas = [t.lemma_.lower() for t in sentence if t.is_alpha]
freq = Counter(lemmas)

sentence_df = pd.DataFrame(freq.items(), columns=["lemma", "count"]).sort_values("count", ascending=False)
sentence_df
```

```{python}
import matplotlib.pyplot as plt

top = sentence_df.head(10)
plt.bar(top["lemma"], top["count"])
plt.title("Top 10 Lemmas in the Text")
plt.xlabel("Lemma")
plt.ylabel("Count")
plt.show()
```

```{python}
import spacy

content_lemmas = [t.lemma_.lower() for t in doc
                  if not (t.is_stop or t.is_punct or t.is_space or t.like_num)]
                  
content_lemmas
```

```{python}
from collections import Counter
import pandas as pd

freq = Counter(content_lemmas)
df_freq = (pd.DataFrame(freq.items(), columns=["lemma", "count"])
           .sort_values("count", ascending=False))
df_freq.head(10)
```

```{python}
import matplotlib.pyplot as plt

top10 = df_freq.head(10)
plt.figure()
plt.bar(top10["lemma"], top10["count"])
plt.title("Top 10 Content Lemmas")
plt.xlabel("Lemma")
plt.ylabel("Count")
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()
```

```{python}
import spacy 
import pandas as pd

# Load English model
nlp = spacy.load("en_core_web_sm")

text2 = "President Pitzer, Mr. Vice President, Governor Connally, ladies and gentlemen: I am delighted to be here today. We meet in an hour of change and challenge."
doc2 = nlp(text2)
[s.text for s in doc2.sents]
```

```{python}
doc = nlp("The quick brown fox jumps over the lazy dog.")
[(t.text, t.dep_, t.head.text) for t in doc]
```

```{python}
import spacy
from spacy import displacy

nlp = spacy.load("en_core_web_sm")
doc = nlp("The quick brown fox jumps over the lazy dog.")

svg = displacy.render(doc, style="dep")  # returns SVG/HTML markup
with open("syntax_tree.svg", "w", encoding="utf-8") as f:
    f.write(svg)
```

```{python}
doc  = "This spaCy library is too dang powerful."
```

```{python}
import spacy

nlp = spacy.load("en_core_web_sm")

doc = nlp("We considered the options and chose the best proposal.")
pairs = []
for tok in doc:
    if tok.pos_ == "VERB":
        dobj = [c for c in tok.children if c.dep_ == "dobj"]
        if dobj:
            pairs.append((tok.lemma_, dobj[0].text))
pairs
```

```{python}
import spacy

nlp = spacy.load("en_core_web_sm")

# Step 1: Our manual list of violent verbs
verbs_of_violence = ["attack", "hit", "kick", "strike", "punch", "assault", "kill", "hurt"]

# Step 2: Process a sentence
doc = nlp("They punched, kicked, and attacked the intruder before fleeing.")

# Step 3: Find any tokens whose lemma is in our list
matches = [(t.text, t.lemma_) for t in doc if t.lemma_ in verbs_of_violence]

print(matches)
```

```{python}
# Semantic Group
dog_words = ["dog", "hound", "terrier", "poodle", "retriever", "shepherd", "beagle", "collie"]

# Text
text = "The farmer owned three terriers, but the poodle ran away with a collie."
```

```{python}
import spacy
nlp = spacy.load("en_core_web_sm")

# Step 1: Define your semantic group
dog_words = ["dog", "hound", "terrier", "poodle", "retriever", "shepherd", "beagle", "collie"]

# Step 2: Sample text
text = "The farmer owned three terriers, but the poodle ran away with a collie."

# Step 3: Process the text
doc = nlp(text)

# Step 4: Collect all nouns that are objects of verbs or prepositions

obj_deps = ["dobj", "pobj", "obj"]
objects = []

for tok in doc:
    if tok.dep_ in obj_deps:
        objects.append(tok)

# Step 5: Keep only those whose lemma is in our semantic group
matches = [(t.text, t.lemma_) for t in objects if t.lemma_.lower() in dog_words]

print(matches)
```

```{python}
# Installing spacy-wordnet
python -m pip install spacy spacy-wordnet nltk

# Installing NLTK wordnet data
python -m nltk.downloader wordnet omw

# Downloading English spaCy model (you should already have this)
python -m spacy download en_core_web_sm
```

```{python}
import spacy
from spacy_wordnet.wordnet_annotator import WordnetAnnotator

nlp = spacy.load("en_core_web_sm")
# Attach the WordNet annotator; it will use the NLTK WordNet data you downloaded
nlp.add_pipe("spacy_wordnet", after="tagger")
```

```{python}
doc = nlp("The dog chased the cat.")
tok = doc[1]

synsets = tok._.wordnet.synsets()   # list of NLTK-style Synset objects

print(f"These are the different meanings the word '{tok}' has:")
count = 0

for i in synsets:
  print(f"{count}: ", i)
  count += 1
```

```{python}
doc = nlp("The dog chased the cat.")
tok = doc[2]

for s in tok._.wordnet.synsets():
    print(s, "→", s.definition())
```

```{python}
for s in tok._.wordnet.synsets():
    print(s, "→", s.examples())
```

```{python}
for s in tok._.wordnet.synsets():
    print(s, "→", [l.name() for l in s.lemmas()])
```

```{python}
import spacy
from nltk.corpus import wordnet as wn
from nltk.corpus.reader.wordnet import NOUN, VERB, ADJ, ADV

# Load spaCy
nlp = spacy.load("en_core_web_sm")

# Sentence with both noun and verb "bear"
text = "The bears bear their burdens bravely."
doc = nlp(text)

# Map spaCy POS tags to WordNet POS tags -- this is a **function**, we'll get to these soon
def get_wordnet_pos(spacy_pos):
    if spacy_pos.startswith("N"):
        return NOUN
    elif spacy_pos.startswith("V"):
        return VERB
    elif spacy_pos.startswith("J"):
        return ADJ
    elif spacy_pos.startswith("R"):
        return ADV
    return None

# Loop through tokens and look up WordNet entries
for token in doc:
    wn_pos = get_wordnet_pos(token.tag_)
    lemma = token.lemma_.lower()

    if wn_pos and not token.is_stop and not token.is_punct:
        synsets = wn.synsets(lemma, pos=wn_pos)
        print(f"\n{token.text.upper()} ({token.pos_}) → lemma: {lemma}")
        for s in synsets[:3]:  # show just the first 3 senses
            print(f"  - {s.definition()}  [examples: {s.examples()}]")
```

```{python}
import spacy
nlp = spacy.load("en_core_web_sm")

text = "Dr. Byrd's students can't wait to analyze PIE roots!"

doc = nlp(text)
[t.text for t in doc]
```

```{python}
import spacy
import pandas as pd

# Load English model
nlp = spacy.load("en_core_web_sm")

text = "Dr. Byrd's students can't wait to analyze PIE roots!"
doc = nlp(text)

# Create list of dicts, one per token
data = []
for t in doc:
    data.append({
        "text": t.text,
        "lemma": t.lemma_,
        "POS": t.pos_,
        "tag": t.tag_,
        "stop": t.is_stop,
        "is_punct": t.is_punct
    })

# Make DataFrame
df = pd.DataFrame(data)
print(df)
```

```{python}
import spacy

content_lemmas = [t.lemma_.lower() for t in doc
                  if not (t.is_stop or t.is_punct or t.is_space or t.like_num)]
```

```{python}
from collections import Counter
import pandas as pd

freq = Counter(content_lemmas)
df_freq = (pd.DataFrame(freq.items(), columns=["lemma", "count"])
           .sort_values("count", ascending=False))
df_freq.head(10)
```

```{python}
import matplotlib.pyplot as plt

top10 = df_freq.head(10)
plt.figure()
plt.bar(top10["lemma"], top10["count"])
plt.title("Top 10 Content Lemmas")
plt.xlabel("Lemma")
plt.ylabel("Count")
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()
```

```{python}
import spacy 
import pandas as pd

# Load English model
nlp = spacy.load("en_core_web_sm")

text2 = "President Pitzer, Mr. Vice President, Governor Connally, ladies and gentlemen: I am delighted to be here today. We meet in an hour of change and challenge."
doc2 = nlp(text2)
[s.text for s in doc2.sents]
```

```{python}
import spacy
from nltk.corpus import wordnet as wn
from nltk.corpus.reader.wordnet import NOUN, VERB, ADJ, ADV

# Load spaCy
nlp = spacy.load("en_core_web_sm")

# Sentence with both noun and verb "bear"
text = "The bears bear their burdens bravely."
doc = nlp(text)

# Map spaCy POS tags to WordNet POS tags -- this is a **function**, we'll get to these soon
def get_wordnet_pos(spacy_pos):
    if spacy_pos.startswith("N"):
        return NOUN
    elif spacy_pos.startswith("V"):
        return VERB
    elif spacy_pos.startswith("J"):
        return ADJ
    elif spacy_pos.startswith("R"):
        return ADV
    return None

# Loop through tokens and look up WordNet entries
for token in doc:
    wn_pos = get_wordnet_pos(token.tag_)
    lemma = token.lemma_.lower()

    if wn_pos and not token.is_stop and not token.is_punct:
        synsets = wn.synsets(lemma, pos=wn_pos)
        print(f"\n{token.text.upper()} ({token.pos_}) → lemma: {lemma}")
        for s in synsets[:3]:  # show just the first 3 senses
            print(f"  - {s.definition()}  [examples: {s.examples()}]")
```

```{python}
import spacy
from spacy_wordnet.wordnet_annotator import WordnetAnnotator

nlp = spacy.load("en_core_web_sm")
# Attach the WordNet annotator; it will use the NLTK WordNet data you downloaded
nlp.add_pipe("spacy_wordnet", after="tagger")
```

Activity in class:

```{python}
getty = "Four score and seven years ago our fathers brought forth on this continent, a new nation, conceived in Liberty, and dedicated to the proposition that all men are created equal.Now we are engaged in a great civil war, testing whether that nation, or any nation so conceived and so dedicated, can long endure. We are met on a great battle-field of that war. We have come to dedicate a portion of that field, as a final resting place for those who here gave their lives that that nation might live. It is altogether fitting and proper that we should do this.But, in a larger sense, we can not dedicate—we can not consecrate—we can not hallow—this ground. The brave men, living and dead, who struggled here, have consecrated it, far above our poor power to add or detract. The world will little note, nor long remember what we say here, but it can never forget what they did here. It is for us the living, rather, to be dedicated here to the unfinished work which they who fought here have thus far so nobly advanced. It is rather for us to be here dedicated to the great task remaining before us—that from these honored dead we take increased devotion to that cause for which they gave the last full measure of devotion—that we here highly resolve that these dead shall not have died in vain—that this nation, under God, shall have a new birth of freedom—and that government of the people, by the people, for the people, shall not perish from the earth."
```

```{python}
import spacy
from collections import Counter
import pandas as pd
import matplotlib.pyplot as plt
nlp = spacy.load("en_core_web_sm")
from nltk.corpus import wordnet as wn
from nltk.corpus.reader.wordnet import NOUN, VERB, ADJ, ADV

doc = nlp(getty)

content_lemmas = [t.lemma_.lower() for t in doc
                  if not (t.is_stop or t.is_punct or t.is_space or t.like_num)]

freq = Counter(content_lemmas)
df_freq = (pd.DataFrame(freq.items(), columns=["lemma", "count"])
           .sort_values("count", ascending=False))
df_freq.head(10)

top10 = df_freq.head(10)
plt.figure()
plt.bar(top10["lemma"], top10["count"])
plt.title("Top 10 Content Lemmas")
plt.xlabel("Lemma")
plt.ylabel("Count")
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

sents = [s.text for s in doc.sents]
first_sent = sents[0]
print(first_sent)

# doc = nlp(text)

first_sent_doc = nlp(first_sent)

# Map spaCy POS tags to WordNet POS tags -- this is a **function**, we'll get to these soon
def get_wordnet_pos(spacy_pos):
    if spacy_pos.startswith("N"):
        return NOUN
    elif spacy_pos.startswith("V"):
        return VERB
    elif spacy_pos.startswith("J"):
        return ADJ
    elif spacy_pos.startswith("R"):
        return ADV
    return None

# Loop through tokens and look up WordNet entries
for token in doc:
    wn_pos = get_wordnet_pos(token.tag_)
    lemma = token.lemma_.lower()

    if wn_pos and not token.is_stop and not token.is_punct:
        synsets = wn.synsets(lemma, pos=wn_pos)
        print(f"\n{token.text.upper()} ({token.pos_}) → lemma: {lemma}")
        for s in synsets[:3]:  # show just the first 3 senses
            print(f"  - {s.definition()}  [examples: {s.examples()}]")
```

```{python}
def get_antonyms(word): # this is also a function
    antonyms = set() # A set is like a list, but prevents duplicates
    for syn in wn.synsets(word):
        for lemma in syn.lemmas():  # analyzes each lemma associated with the token
            for ant in lemma.antonyms():
                antonyms.add(ant.name())
    return list(antonyms)

get_antonyms("slow")
```

```{python}
import spacy
from spacy.tokens import Token
from nltk.corpus import wordnet as wn

nlp = spacy.load("en_core_web_sm")

# Register a getter-based extension
def get_synsets(token):
    # You can filter by POS if you want, e.g., pos=wn.NOUN for nouns
    return wn.synsets(token.lemma_.lower())

Token.set_extension("synsets", getter=get_synsets, force=True)

doc = nlp("dog")
token = doc[0]

word_synsets = token._.synsets  # now this works
for synset in word_synsets:
    hypernym_names = [h.name() for h in synset.hypernyms()]
    hyponym_names  = [h.name() for h in synset.hyponyms()]
    print(f"\nSense: {synset.name()}")
    print(f"  Hypernyms: {hypernym_names}")
    print(f"  Hyponyms:  {hyponym_names}")
```

```{python}
import spacy
from spacy.tokens import Token
from nltk.corpus import wordnet as wn

# Creating a function to "get" synsets
def get_synsets(token):
    # You can filter by POS if you want, e.g., pos=wn.NOUN for nouns
    return wn.synsets(token.lemma_.lower())

Token.set_extension("synsets", getter=get_synsets, force=True)

doc = nlp("dog")
token = doc[0]

word_synsets = token._.synsets  # now this works
for synset in word_synsets:
    hypernym_names = [h.name() for h in synset.hypernyms()]
    hyponym_names  = [h.name() for h in synset.hyponyms()]
    mero_names = [h.name() for h in synset.part_meronyms()]
    holo_names = [h.name() for h in synset.part_holonyms()]
    print(f"\nSense: {synset.name()}")
    print(f"  Hypernyms: {hypernym_names}")
    print(f"  Hyponyms:  {hyponym_names}")
    print(f"  Part Meronyms: {mero_names}")
    print(f"  Part Holonyms:  {holo_names}")
```

```{python}
import spacy
from spacy.tokens import Token
from nltk.corpus import wordnet as wn

# Creating a function to "get" synsets
def get_synsets(token):
    # You can filter by POS if you want, e.g., pos=wn.NOUN for nouns
    return wn.synsets(token.lemma_.lower())

Token.set_extension("synsets", getter=get_synsets, force=True)

doc = nlp("cat")
token = doc[0]

word_synsets = token._.synsets  # now this works
for synset in word_synsets:
    hypernym_names = [h.name() for h in synset.hypernyms()]
    hyponym_names  = [h.name() for h in synset.hyponyms()]
    mero_names = [h.name() for h in synset.part_meronyms()]
    holo_names = [h.name() for h in synset.part_holonyms()]
    ent_names = [h.name() for h in synset.entailments()]
    print(f"\nSense: {synset.name()}")
    print(f"  Hypernyms: {hypernym_names}")
    print(f"  Hyponyms:  {hyponym_names}")
    print(f"  Part Meronyms: {mero_names}")
    print(f"  Part Holonyms:  {holo_names}")
    print(f"  Entailments:  {ent_names}")
```

```{python}
dog = nlp("dog")[0]._.wordnet.synsets()[0]
comparison = nlp("cat")[0]._.wordnet.synsets()[0]

print("wup:",  dog.wup_similarity(comparison))   # Wu–Palmer similarity
```

```{python}
import spacy
from spacy.matcher import PhraseMatcher
from nltk.corpus import wordnet as wn

nlp = spacy.load("en_core_web_sm")

# WordNet helpers (via NLTK) – returns only verb synsets
def verb_senses(word):
    return wn.synsets(word, pos='v')  # verb synsets only

# Collects all (recursive) hyponyms for a verb synset (troponyms in WN terms).
# This function recursively collects all *hyponyms* (a.k.a. “troponyms” for verbs)
# of a given synset. A hyponym is a more *specific* instance of an action.
# Example: the verb “attack.v.01” has hyponyms like “bomb.v.01”, “invade.v.01”, etc.
def all_verb_hyponyms(root):
      seen, stack = set(), [root]
    while stack:
        cur = stack.pop()
        if cur in seen:
            continue
        seen.add(cur)
        # For verbs, .hyponyms() are the troponyms
        stack.extend(cur.hyponyms())
    return {s for s in seen if s.pos() == 'v'}

# This function extracts lemma names (the “canonical” word forms)
# from a set of synsets, e.g. 'strike.v.01' → ['strike', 'hit', 'smite', ...].
# Optionally filters out multiword expressions like "shoot_down".
def lemmas_from_synsets(synsets, keep_multiword=False):
    out = set()
    for s in synsets:
        for lem in s.lemmas():
            name = lem.name().lower()
            if not keep_multiword and "_" in name:
                continue
            out.add(name.replace("_", " "))
    return out

# Build a violence lexicon from WordNet using a few intuitive seeds
seed_verbs = ["attack", "assault", "hit", "strike", "punch", "kick", "stab", "shoot", "beat"]
base_synsets = []
for w in seed_verbs:
    ss = verb_senses(w)
    if ss:
        # take the most “central” sense by picking the one with most hyponyms
        ss_scored = sorted(ss, key=lambda s: len(s.hyponyms()), reverse=True)
        base_synsets.append(ss_scored[0])

# expand via hyponyms (troponyms)
expanded = set()
for s in base_synsets:
    expanded |= all_verb_hyponyms(s)

# collect lemmas (single-token by default)
violent_verb_lemmas = sorted(lemmas_from_synsets(expanded, keep_multiword=False) | set(seed_verbs))
print(f"{len(violent_verb_lemmas)} violent verb lemmas (sample):", violent_verb_lemmas[:25])

# 4) spaCy PhraseMatcher by lemma — IMPORTANT: run full pipeline on patterns
matcher = PhraseMatcher(nlp.vocab, attr="LEMMA")
patterns = list(nlp.pipe(violent_verb_lemmas))  # not make_doc: we need lemmas
matcher.add("VIOLENCE", patterns)
```

```{python}
import spacy
nlp_en = spacy.load("en_core_web_sm")

doc = nlp_en("The children ran quickly to their houses.")
for token in doc:
    print(token.text, token.lemma_, token.pos_)
```

```{python}
import spacy
from nltk.corpus import wordnet as wn
from nltk.corpus.reader.wordnet import NOUN, VERB, ADJ, ADV

# Load spaCy
nlp = spacy.load("en_core_web_sm")

# Sentence with both noun and verb "bear"
text = "The bears bear their burdens bravely."
doc = nlp(text)

# Map spaCy POS tags to WordNet POS tags -- this is a **function**, we'll get to these soon
def get_wordnet_pos(spacy_pos):
    if spacy_pos.startswith("N"):
        return NOUN
    elif spacy_pos.startswith("V"):
        return VERB
    elif spacy_pos.startswith("J"):
        return ADJ
    elif spacy_pos.startswith("R"):
        return ADV
    return None

# Loop through tokens and look up WordNet entries
for token in doc:
    wn_pos = get_wordnet_pos(token.tag_)
    lemma = token.lemma_.lower()

    if wn_pos and not token.is_stop and not token.is_punct:
        synsets = wn.synsets(lemma, pos=wn_pos)
        print(f"\n{token.text.upper()} ({token.pos_}) → lemma: {lemma}")
        for s in synsets[:3]:  # show just the first 3 senses
            print(f"  - {s.definition()}  [examples: {s.examples()}]")
```

```{python}
dog = nlp("dog")[0]._.wordnet.synsets()[0]
comparison = nlp("cat")[0]._.wordnet.synsets()[0]

print("wup:",  dog.wup_similarity(comparison))   # Wu–Palmer similarity
```

```{python}
import spacy
from spacy.matcher import PhraseMatcher
from nltk.corpus import wordnet as wn

nlp = spacy.load("en_core_web_sm")

# WordNet helpers (via NLTK) – returns only verb synsets
def verb_senses(word):
    return wn.synsets(word, pos='v')  # verb synsets only

# Collects all (recursive) hyponyms for a verb synset (troponyms in WN terms).
# This function recursively collects all *hyponyms* (a.k.a. “troponyms” for verbs)
# of a given synset. A hyponym is a more *specific* instance of an action.
# Example: the verb “attack.v.01” has hyponyms like “bomb.v.01”, “invade.v.01”, etc.
def all_verb_hyponyms(root):
      seen, stack = set(), [root]
    while stack:
        cur = stack.pop()
        if cur in seen:
            continue
        seen.add(cur)
        # For verbs, .hyponyms() are the troponyms
        stack.extend(cur.hyponyms())
    return {s for s in seen if s.pos() == 'v'}

# This function extracts lemma names (the “canonical” word forms)
# from a set of synsets, e.g. 'strike.v.01' → ['strike', 'hit', 'smite', ...].
# Optionally filters out multiword expressions like "shoot_down".
def lemmas_from_synsets(synsets, keep_multiword=False):
    out = set()
    for s in synsets:
        for lem in s.lemmas():
            name = lem.name().lower()
            if not keep_multiword and "_" in name:
                continue
            out.add(name.replace("_", " "))
    return out

# Build a violence lexicon from WordNet using a few intuitive seeds
seed_verbs = ["attack", "assault", "hit", "strike", "punch", "kick", "stab", "shoot", "beat"]
base_synsets = []
for w in seed_verbs:
    ss = verb_senses(w)
    if ss:
        # take the most “central” sense by picking the one with most hyponyms
        ss_scored = sorted(ss, key=lambda s: len(s.hyponyms()), reverse=True)
        base_synsets.append(ss_scored[0])

# expand via hyponyms (troponyms)
expanded = set()
for s in base_synsets:
    expanded |= all_verb_hyponyms(s)

# collect lemmas (single-token by default)
violent_verb_lemmas = sorted(lemmas_from_synsets(expanded, keep_multiword=False) | set(seed_verbs))
print(f"{len(violent_verb_lemmas)} violent verb lemmas (sample):", violent_verb_lemmas[:25])

# 4) spaCy PhraseMatcher by lemma — IMPORTANT: run full pipeline on patterns
matcher = PhraseMatcher(nlp.vocab, attr="LEMMA")
patterns = list(nlp.pipe(violent_verb_lemmas))  # not make_doc: we need lemmas
matcher.add("VIOLENCE", patterns)
```

```{python}
import spacy
from spacy.matcher import PhraseMatcher

nlp = spacy.load("en_core_web_sm")

violent_vs = ['assail', 'assault', 'atom-bomb', 'atomise', 'atomize', 'attack', 'backbite', 'backhand', 'bait', 'bastinado', 'bat', 'batter', 'bayonet', 'beak', 'beat', 'beef', 'beetle', 'beleaguer', 'bellyache', 'bemoan', 'beset', 'besiege', 'best', 'better', 'bewail', 'birdie', 'bitch', 'blast', 'bleat', 'blindside', 'blitz', 'blockade', 'bogey', 'bomb', 'bombard', 'bounce', 'break', 'buffet', 'bulldog', 'bunker', 'bunt', 'bust', 'butt', 'cannon', 'cannonade', 'carom', 'carry', 'charge', 'cheat', 'checkmate', 'chicane', 'chip', 'chop', 'chouse', 'circumvent', 'clap', 'clobber', 'clout', 'coldcock', 'complain', 'connect', 'counterattack', 'counterstrike', 'crab', 'cream', 'croak', 'croquet', 'crump', 'crush', 'cuff', 'dab', 'deck', 'declaim', 'deplore', 'desecrate', 'dishonor', 'dishonour', 'dive-bomb', 'double', 'down', 'dribble', 'drive', 'drub', 'dump', 'dunk', 'eagle', 'ebb', 'eliminate', 'exceed', 'firebomb', 'floor', 'fly', 'foul', 'full', 'gang-rape', 'gas', 'glide-bomb', 'gnarl', 'gripe', 'grizzle', 'grouch', 'ground', 'grouse', 'grumble', 'hammer', 'headbutt', 'heel', 'hen-peck', 'hew', 'hit', 'hole', 'holler', 'hook', 'hydrogen-bomb', 'immobilise', 'immobilize', 'infest', 'invade', 'inveigh', 'jab', 'jockey', 'jump', 'kick', 'kill', 'knap', 'knife', 'knock', 'knuckle', 'kvetch', 'lament', 'lash', 'lick', 'loft', 'master', 'mate', 'molest', 'murmur', 'mutter', 'nag', 'nuke', 'occupy', 'out-herod', 'outbrave', 'outcry', 'outdo', 'outdraw', 'outfight', 'outflank', 'outfox', 'outgeneral', 'outgo', 'outgrow', 'outmaneuver', 'outmanoeuvre', 'outmarch', 'outmatch', 'outpace', 'outperform', 'outplay', 'outpoint', 'outrage', 'outrange', 'outroar', 'outsail', 'outscore', 'outsell', 'outshine', 'outshout', 'outsmart', 'outstrip', 'outwear', 'outweigh', 'outwit', 'overcome', 'overmaster', 'overpower', 'overreach', 'overrun', 'overwhelm', 'paste', 'pat', 'pattern-bomb', 'peck', 'pelt', 'pepper', 'percuss', 'pick', 'pip', 'pitch', 'plain', 'play', 'plug', 'poniard', 'pop', 'profane', 'protest', 'pull', 'punch', 'putt', 'quetch', 'racket', 'raid', 'rail', 'rap', 'rape', 'ravish', 'reassail', 'repine', 'report', 'retaliate', 'rout', 'rush', 'savage', 'sclaff', 'scold', 'scoop', 'screw', 'set', 'shaft', 'shame', 'shank', 'shell', 'shoot', 'sic', 'sideswipe', 'single', 'skip-bomb', 'slam-dunk', 'slap', 'sledge', 'sledgehammer', 'slice', 'smash', 'snag', 'snap', 'snick', 'spread-eagle', 'spreadeagle', 'spur', 'squawk', 'stab', 'steamroll', 'steamroller', 'storm', 'strafe', 'strike', 'stroke', 'subdue', 'submarine', 'surmount', 'surpass', 'surprise', 'surround', 'tap', 'teargas', 'thrash', 'thresh', 'tip', 'toe', 'top', 'torpedo', 'triple', 'trounce', 'trump', 'undercut', 'upstage', 'urticate', 'vanquish', 'violate', 'volley', 'whang', 'whine', 'whip', 'whomp', 'worst', 'yammer', 'yawp', 'zap']

patterns = [nlp(v) for v in violent_vs]
matcher = PhraseMatcher(nlp.vocab, attr="LEMMA")
doc = nlp("They slapped, punched, kicked, and attacked the intruder before fleeing.")
matcher.add("VIOLENCE", patterns)
[(doc[s:e].text, doc[s:e].lemma_) for _, s, e in matcher(doc)]
```

```{python}
import spacy

nlp_en = spacy.load("en_core_web_sm")
nlp_fr = spacy.load("fr_core_news_sm")
nlp_es = spacy.load("es_core_news_sm")
nlp_de = spacy.load("de_core_news_sm")

text_en = "The students have analyzed Proto-Indo-European roots."
text_fr = "Les étudiants ont analysé les racines proto-indo-européennes."
text_es = "Los estudiantes analizaron las raíces protoindoeuropeas."
text_de = "Die Studenten analysierten die indogermanischen Wurzeln."

for lang, nlp, text in [("English", nlp_en, text_en), ("French", nlp_fr, text_fr), ("Spanish", nlp_es, text_es), ("German", nlp_de, text_de)]:
    doc = nlp(text)
    print(f"\n {lang} Tokens:")
    print([t.text for t in doc])
```

```{python}
import spacy

nlp_en = spacy.load("en_core_web_sm")
nlp_fr = spacy.load("fr_core_news_sm")
nlp_es = spacy.load("es_core_news_sm")
nlp_de = spacy.load("de_core_news_sm")

text_en = "It is snowing today. I'm cold."
text_fr = "Il neige aujourd'hui. J'ai froid."
text_es = "Hoy está nevando. Tengo frío."
text_de = "Es schneit heute. Mir ist kalt."

for lang, nlp, text in [("English", nlp_en, text_en), ("French", nlp_fr, text_fr), ("Spanish", nlp_es, text_es), ("German", nlp_de, text_de)]:
    doc = nlp(text)
    print(f"\n {lang} Tokens:")
    print([t.text for t in doc])
```

```{python}
for lang, nlp, text in [("English", nlp_en, text_en), ("French", nlp_fr, text_fr), ("Spanish", nlp_es, text_es), ("German", nlp_de, text_de)]:
    doc = nlp(text)
    print(f"\n {lang} Lemmas:")
    for token in doc:
        print(token.text, "→", token.lemma_, token.pos_)
```

```{python}
import pandas as pd

def lemma_table(nlp, text, language):
    doc = nlp(text)
    data = [(t.text, t.lemma_, t.pos_) for t in doc if t.is_alpha]
    return pd.DataFrame(data, columns=["word", "lemma", "pos"]).assign(language=language)

df_all = pd.concat([
    lemma_table(nlp_en, text_en, "English"),
    lemma_table(nlp_fr, text_fr, "French"),
    lemma_table(nlp_es, text_es, "Spanish"),
    lemma_table(nlp_de, text_de, "German")
])

df_all
```

```{python}
text_en = "Whereas recognition of the inherent dignity and of the equal and inalienable rights of all members of the human family is the foundation of freedom, justice and peace in the world, the peoples of the United Nations have reaffirmed their faith in fundamental human rights and in the dignity and worth of the human person. They have resolved to promote social progress and better standards of life in larger freedom."

text_es = "Considerando que el reconocimiento de la dignidad intrínseca y de los derechos iguales e inalienables de todos los miembros de la familia humana constituye la base de la libertad, la justicia y la paz en el mundo, los pueblos de las Naciones Unidas han reafirmado su fe en los derechos humanos fundamentales y en la dignidad y el valor de la persona humana. Han decidido promover el progreso social y elevar el nivel de vida dentro de una libertad más amplia."
```

```{python}
from collections import Counter
import spacy

nlp_en = spacy.load("en_core_web_sm")
nlp_es = spacy.load("es_core_news_sm")

def is_def_det(tok, include_contractions=False, lang="en"):
# True for tokens that are definite determiners
  if tok.pos_ == "DET" and "Def" in tok.morph.get("Definite"):
    return True
  if include_contractions and lang == "es" and tok.text.lower() in {"al", "del"}:
    return True
    return False

def def_det_stats(nlp, text, lang_label, include_contractions=False):
  doc = nlp(text)
  # word-like tokens only for the denominator
  word_tokens = [t for t in doc if t.is_alpha]
  total = len(word_tokens)
  def_dets = [t.text for t in doc if is_def_det(t, include_contractions, lang=lang_label[:2].lower())]
  n = len(def_dets)
  rate = (n / total) if total else 0.0
  print(f"{lang_label}: {n} definite determiners / {total} word tokens ({rate:.2%})")
  print("Top forms:", Counter(w.lower() for w in def_dets).most_common())

# Run (set include_contractions=True if you want to count 'al'/'del' in Spanish)
def_det_stats(nlp_en, text_en, "English", include_contractions=False)
def_det_stats(nlp_es, text_es, "Spanish", include_contractions=False)
```

```{python}
# pip install pycountry
import pycountry
from nltk.corpus import wordnet as wn
# List languages available via OMW (IDs)
codes = sorted(wn.langs())

for code in codes:
    lang = pycountry.languages.get(alpha_3=code)
    if lang:
        print(f"{code} → {lang.name}")
    else:
        print(f"{code} → (not found)")
```

```{python}
# Example: Spanish lemmas linked to the English synset for 'dog.n.01'
ss = wn.synset('dog.n.01')
[lem.name() for lem in ss.lemmas(lang='spa')]

ss = wn.synset('dog.n.01')
[lem.name() for lem in ss.lemmas(lang='fra')]
```

```{python}
from collections import Counter
import spacy

nlp_en = spacy.load("en_core_web_sm")
nlp_es = spacy.load("es_core_news_sm")

def is_preposition(tok):
  # True for tokens that are prepositions or postpositions
  return tok.pos_ == "ADP"

def prep_stats(nlp, text, lang_label):
  doc = nlp(text)
  # word-like tokens only for the denominator
  word_tokens = [t for t in doc if t.is_alpha]
  total = len(word_tokens)
  preps = [t.text for t in doc if is_preposition(t)]
  n = len(preps)
  rate = (n / total) if total else 0.0
  print(f"{lang_label}: {n} prepositions / {total} word tokens ({rate:.2%})")
  print("Top forms:", Counter(w.lower() for w in preps).most_common())

# Run on English and Spanish samples
prep_stats(nlp_en, text_en, "English")
prep_stats(nlp_es, text_es, "Spanish")
```

```{python}
text_fr = "Considérant que la reconnaissance de la dignité inhérente et des droits égaux et inaliénables de tous les membres de la famille humaine constitue le fondement de la liberté, de la justice et de la paix dans le monde, les peuples des Nations Unies ont réaffirmé leur foi dans les droits fondamentaux de l’homme, dans la dignité et la valeur de la personne humaine. Ils se sont engagés à favoriser le progrès social et à élever le niveau de vie dans une liberté plus grande."

text_de = "Da die Anerkennung der angeborenen Würde und der gleichen und unveräußerlichen Rechte aller Mitglieder der menschlichen Familie die Grundlage der Freiheit, der Gerechtigkeit und des Friedens in der Welt bildet, haben die Völker der Vereinten Nationen ihren Glauben an die grundlegenden Menschenrechte sowie an die Würde und den Wert der menschlichen Person erneut bekräftigt. Sie haben beschlossen, sozialen Fortschritt zu fördern und den Lebensstandard in größerer Freiheit zu erhöhen."
```

```{python}
from collections import Counter
import spacy

nlp_fr = spacy.load("fr_core_news_sm")
nlp_de = spacy.load("de_core_news_sm")

def is_preposition(tok):
  # True for tokens that are prepositions or postpositions
  return tok.pos_ == "ADP"

def prep_stats(nlp, text, lang_label):
  doc = nlp(text)
  # word-like tokens only for the denominator
  word_tokens = [t for t in doc if t.is_alpha]
  total = len(word_tokens)
  preps = [t.text for t in doc if is_preposition(t)]
  n = len(preps)
  rate = (n / total) if total else 0.0
  print(f"{lang_label}: {n} prepositions / {total} word tokens ({rate:.2%})")
  print("Top forms:", Counter(w.lower() for w in preps).most_common())

# Run on English and Spanish samples
prep_stats(nlp_fr, text_fr, "French")
prep_stats(nlp_de, text_de, "German")
```

```{python}
import spacy

el_text = "Ο σκύλος τρέχει γρήγορα στον κήπο. Οι σκύλοι είναι πιστά ζώα."
zh_text = "狗在花园里跑得很快。狗是忠诚的动物。"
ja_text = "犬は庭で速く走ります。犬は忠実な動物です。"

nlp_el = spacy.load("el_core_news_sm")
nlp_zh = spacy.load("zh_core_web_sm")
nlp_ja = spacy.load("ja_core_news_sm")

for lang, nlp, text in [
    ("Greek", nlp_el, el_text),
    ("Chinese", nlp_zh, zh_text),
    ("Japanese", nlp_ja, ja_text)
]:
    doc = nlp(text)
    print(f"\n{lang} tokens:")
    print([t.text for t in doc])
```

```{python}
import spacy

nlp_el = spacy.load("el_core_news_sm")
nlp_zh = spacy.load("zh_core_web_sm")
nlp_ja = spacy.load("ja_core_news_sm")

for lang, nlp, text in [
    ("Greek", nlp_el, el_text),
    ("Chinese", nlp_zh, zh_text),
    ("Japanese", nlp_ja, ja_text)
]:
    doc = nlp(text)
    print(f"\n{lang} lemmas:")
    print([t.lemma_ for t in doc])
```

```{python}
import spacy

nlp_el = spacy.load("el_core_news_sm")
nlp_zh = spacy.load("zh_core_web_sm")
nlp_ja = spacy.load("ja_core_news_sm")

cat_el = "Η γάτα κοιμάται στον καναπέ."
cat_zh = "猫在沙发上睡觉。"
cat_ja = "猫はソファで寝ます。"

for lang, nlp, text in [
    ("Chinese", nlp_zh, cat_zh)
]:
    doc = nlp(text)
    print(f"\n{lang} lemmas:")
    print([t.text for t in doc])
    
for lang, nlp, text in [
    ("Greek", nlp_el, cat_el),
    ("Japanese", nlp_ja, cat_ja)
]:
    doc = nlp(text)
    print(f"\n{lang} lemmas:")
    print([t.lemma_ for t in doc])
```

```{python}
# pip install Unidecode
from unidecode import unidecode

el_text = "Ο σκύλος τρέχει γρήγορα στον κήπο. Οι σκύλοι είναι πιστά ζώα."
print(unidecode(el_text))
```

```{python}
# pip install pypinyin
from pypinyin import pinyin, Style

zh_text = "狗在花园里跑得很快。狗是忠诚的动物。"
pinyin_list = pinyin(zh_text, style=Style.TONE3)
print(" ".join([syll[0] for syll in pinyin_list]))
```

```{python}
# pip install pykakasi
import pykakasi

text = "犬は庭で速く走ります。犬は忠実な動物です。"

kks = pykakasi.kakasi()
result = kks.convert(text)
romaji = " ".join([item['hepburn'] for item in result])
print(romaji)
```

```{python}
import pycountry
from nltk.corpus import wordnet as wn
# List languages available via OMW (IDs)
codes = sorted(wn.langs())

for code in codes:
    lang = pycountry.languages.get(alpha_3=code)
    if lang:
        print(f"{code} → {lang.name}")
    else:
        print(f"{code} → (not found)")
        
# ell = Modern Greek
# cmn = Mandarin
# jpn = Japanese
```

```{python}
from unidecode import unidecode

ss = wn.synset('cat.n.01')
ell_cat = [lem.name() for lem in ss.lemmas(lang='ell')]
cmn_cat = [lem.name() for lem in ss.lemmas(lang='cmn')]
jpn_cat = [lem.name() for lem in ss.lemmas(lang='jpn')]

ell_cat
cmn_cat
jpn_cat
```

```{python}
from unidecode import unidecode
```

```{python}
from pypinyin import pinyin, Style

pinyin_list = pinyin(cmn_cat, style=Style.TONE3)
print(" ".join([syll[0] for syll in pinyin_list]))
```

```{python}
from collections import Counter
import spacy

nlp_en = spacy.load("en_core_web_sm")
nlp_es = spacy.load("es_core_news_sm")

def is_preposition(tok):
  # True for tokens that are prepositions or postpositions
  return tok.pos_ == "ADP"

def prep_stats(nlp, text, lang_label):
  doc = nlp(text)
  # word-like tokens only for the denominator
  word_tokens = [t for t in doc if t.is_alpha]
  total = len(word_tokens)
  preps = [t.text for t in doc if is_preposition(t)]
  n = len(preps)
  rate = (n / total) if total else 0.0
  print(f"{lang_label}: {n} prepositions / {total} word tokens ({rate:.2%})")
  print("Top forms:", Counter(w.lower() for w in preps).most_common())

# Run on English and Spanish samples
prep_stats(nlp_en, text_en, "English")
prep_stats(nlp_es, text_es, "Spanish")
```

```{python}
def plus_two(number):
    '''
    This function adds 2 to the input.
    '''
    output = number + 2
    return(output)
  
plus_two(1)
```

```{python}
def add_and_mult(a, b):
    '''
    This function adds a to b,
    and multiplies a and b.
    '''
    add = a + b
    mult = a * b
    return(add, mult)
  
add_and_mult(4, 6)
```

```{python}
def initialize(full_name):
    '''
    Return the person's initials.
    '''
    name_parts = full_name.split(" ")
    initial_list = [name[0]+"." for name in name_parts]

    output = ""
    for initial in initial_list:
        output = output + initial

    return(output)
  
initialize("Lexie Hayes")

initialize("Tom Henderson")

initialize("Yuzu Hayes")
```

```{python}
from initializer import initialize

initialize("Louisa May Alcott")
# 'L.M.A.'

initialize("Edgar Allan Poe")
# 'E.A.P.'

initialize("Jane Austen")
# 'J.A.'

#getting error because i didn't make the initializer.py file
```

```{python}
# activity to clean up code 
def read_book(book):
  
  book_file = open(book, mode = 'r')
  book_lines = book_file.readlines()
  clean_lines = [line.rstrip().lower() for line in book_lines]
  output = [w for w in clean_lines if w]
  return(output)

pride = "austen/pride_and_prejudice.txt"
sense = "austen/sense_and_sensibility.txt"

read_book(pride)
# error because i dont have the files again lol, but if i did it should work
```

```{python}
def read_book(path):
    with open(path, mode="r", encoding="utf-8") as book_file:
        book_lines = book_file.readlines()
    clean_lines = [line.strip().lower() for line in book_lines]
    
    # Split each line into words and flatten the list
    words = [word for line in clean_lines for word in line.split() if word]
    return words

alice = read_book("alice.txt")
print(alice[:200])
```

```{python}
import spacy

content_lemmas = [t.lemma_.lower() for t in doc
                  if not (t.is_stop or t.is_punct or t.is_space or t.like_num)]
```
